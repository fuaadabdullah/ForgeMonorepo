# Overmind Phase 3: Ollama Integration & Enhanced Capabilities

**Status**: Complete
**Version**: 3.0.0
**Date**: October 25, 2025

## Executive Summary

Phase 3 transforms Overmind into a **local-first, multi-provider AI orchestration system** with:

- âœ… **4 LLM Providers**: Ollama (local), DeepSeek, Gemini, OpenAI
- âœ… **Smart Routing**: Cost-optimized, latency-optimized, local-first, cascading strategies
- âœ… **Tool Calling**: Unified interface with automatic multi-turn execution
- âœ… **Local RAG**: Embeddings, vector store, document chunking, context injection
- âœ… **3 Integration Paths**: OpenAI-compatible, Native, LiteLLM proxy

**Zero API cost development** with Ollama's local inference.

---

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Provider Comparison](#provider-comparison)
3. [Integration Paths](#integration-paths)
4. [Routing Strategies](#routing-strategies)
5. [Tool Calling System](#tool-calling-system)
6. [RAG Pipeline](#rag-pipeline)
7. [Installation & Setup](#installation--setup)
8. [Migration from Phase 2](#migration-from-phase-2)
9. [Usage Examples](#usage-examples)
10. [Troubleshooting](#troubleshooting)
11. [Performance Benchmarks](#performance-benchmarks)
12. [API Reference](#api-reference)

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Overmind Router                        â”‚
â”‚  - Smart provider selection (cost/latency/local-first)     â”‚
â”‚  - Health-aware fallback chains                            â”‚
â”‚  - Task-based model routing                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Adaptersâ”‚    â”‚  LiteLLM Proxy   â”‚   â”‚   Tools    â”‚
â”‚ - OpenAI-compatâ”‚    â”‚  (Unified API)   â”‚   â”‚  Calling   â”‚
â”‚ - Native       â”‚    â”‚                  â”‚   â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚    Local Ollama (11434)           â”‚    â”‚    RAG     â”‚
    â”‚  - llama3.1 (general chat)        â”‚    â”‚  Pipeline  â”‚
    â”‚  - qwen2.5-coder (code + tools)   â”‚    â”‚            â”‚
    â”‚  - nomic-embed-text (embeddings)  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Cloud Providers (via LiteLLM)    â”‚
    â”‚  - DeepSeek (cheap, fast)          â”‚
    â”‚  - Gemini (creative, vision)       â”‚
    â”‚  - OpenAI (highest quality)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Purpose | Files |
|-----------|---------|-------|
| **Router Policy** | Smart provider selection | `src/router/policy.ts` |
| **Ollama Adapters** | Local LLM integration | `src/clients/ollama-*.ts` |
| **LiteLLM Proxy** | Unified cloud gateway | `src/clients/litellm-proxy.ts` |
| **Tool System** | Function calling framework | `src/tools/` |
| **RAG Pipeline** | Local knowledge retrieval | `src/rag/` |

---

## Provider Comparison

### Feature Matrix

| Provider | Local | JSON | Tools | Vision | Avg Latency | Cost (1M tokens) | Use Case |
|----------|-------|------|-------|--------|-------------|------------------|----------|
| **Ollama** | âœ… | âœ… | âœ… | âŒ | 2000ms | $0.00 | Development, privacy-first |
| **DeepSeek** | âŒ | âœ… | âœ… | âŒ | 800ms | $0.14 | Quick tasks, cost-optimized |
| **Gemini** | âŒ | âœ… | âœ… | âœ… | 1200ms | $0.30 | Creative, vision, multi-tool |
| **OpenAI** | âŒ | âœ… | âœ… | âœ… | 1500ms | $2.50 | Highest quality, strategic |
| **LiteLLM** | âŒ | âœ… | âœ… | âœ… | 1000ms | $1.00 | Unified interface, routing |

### Model Recommendations by Task

| Task Type | Primary | Fallback | Reason |
|-----------|---------|----------|--------|
| **Code** | Ollama (qwen2.5-coder) | DeepSeek coder | Specialized code models |
| **Quick** | DeepSeek chat | Ollama (llama3.1) | Speed + cost |
| **Creative** | Gemini flash | OpenAI gpt-4o | Creative capability |
| **RAG/Embeddings** | Ollama (nomic-embed) | OpenAI embedding | Local-first, zero cost |
| **General** | Ollama (llama3.1) | DeepSeek chat | Balanced performance |

---

## Integration Paths

Phase 3 provides **three ways** to integrate with Ollama:

### 1. OpenAI-Compatible (`ollama-openai.ts`)

**Best for**: Drop-in replacement, backwards compatibility

```typescript
import { chatOllama } from './clients/ollama-openai';

const response = await chatOllama([
  { role: 'user', content: 'Explain async/await' }
], 'llama3.1');

console.log(response.content);
```

**Pros**: No code changes, OpenAI SDK familiarity
**Cons**: Limited to OpenAI API features

### 2. Native Client (`ollama-native.ts`)

**Best for**: Advanced features (structured outputs, tool calling, embeddings)

```typescript
import { chatOllamaStructured, chatOllamaTools, embedOllama } from './clients/ollama-native';

// Structured outputs with JSON Schema
const person = await chatOllamaStructured({
  prompt: 'Extract: John is 30 years old',
  schema: {
    type: 'object',
    properties: {
      name: { type: 'string' },
      age: { type: 'number' }
    }
  }
});

// Tool calling
const toolResponse = await chatOllamaTools({
  messages: [{ role: 'user', content: 'What\'s the weather?' }],
  tools: [weatherTool]
});

// Embeddings for RAG
const vector = await embedOllama('search query', 'nomic-embed-text');
```

**Pros**: Full feature access, type-safe
**Cons**: Ollama-specific API

### 3. LiteLLM Proxy (`litellm-proxy.ts`) â­ **RECOMMENDED**

**Best for**: Production, unified interface across all providers

```typescript
import { selectModel, chatLiteLLM } from './clients/litellm-proxy';

// Smart model selection
const model = selectModel({
  taskType: 'code',
  requireJSON: true,
  offlineOnly: false,  // Allow cloud providers
  maxCost: 0.50
});

const response = await chatLiteLLM(messages, {
  taskType: 'code',
  preferProvider: 'ollama'
});
```

**Pros**: Single API for all providers, smart routing, automatic fallbacks
**Cons**: Requires LiteLLM proxy running

---

## Routing Strategies

### Available Strategies

```typescript
export enum RoutingStrategy {
  COST_OPTIMIZED = 'cost-optimized',      // Choose cheapest provider
  LATENCY_OPTIMIZED = 'latency-optimized', // Choose fastest provider
  LOCAL_FIRST = 'local-first',             // Prefer Ollama when available
  CASCADING = 'cascading',                 // Escalate by complexity
  PREDICTIVE = 'predictive',               // Task-based selection
  ROUND_ROBIN = 'round-robin'              // Load balancing
}
```

### Strategy Decision Trees

#### LOCAL_FIRST Strategy
```
Query â†’ Is Ollama healthy?
         â†“ Yes              â†“ No
    Use Ollama          Use cloud fallback
                        (DeepSeek â†’ Gemini â†’ OpenAI)
```

#### CASCADING Strategy
```
Query â†’ Assess complexity
         â†“
    Simple/Moderate â†’ DeepSeek (fast, cheap)
         â†“
    Complex â†’ Gemini (creative, multi-modal)
         â†“
    Strategic â†’ OpenAI (highest quality)
```

#### PREDICTIVE Strategy
```
Query â†’ Extract task type
         â†“
    Code â†’ Ollama qwen2.5-coder / DeepSeek coder
    Quick â†’ DeepSeek chat / Ollama llama3.1
    Creative â†’ Gemini flash / OpenAI gpt-4o
    RAG â†’ Ollama nomic-embed / OpenAI embedding
```

### Configuration

```bash
# .env
OVERMIND_ROUTING_STRATEGY=local-first
OVERMIND_PREFER_LOCAL=true
OVERMIND_OFFLINE_MODE=false      # Force Ollama-only when true
OVERMIND_ENABLE_FALLBACK=true
```

---

## Tool Calling System

### Architecture

```typescript
// 1. Define tool
const weatherTool: ToolDefinition = {
  name: 'get_weather',
  description: 'Get current weather for a location',
  parameters: {
    type: 'object',
    properties: {
      location: { type: 'string', description: 'City name' },
      units: { type: 'string', enum: ['celsius', 'fahrenheit'] }
    },
    required: ['location']
  },
  handler: async (args) => {
    const weather = await fetchWeather(args.location, args.units);
    return JSON.stringify(weather);
  }
};

// 2. Use with automatic multi-turn execution
const result = await toolEnabledChat(
  { messages, tools: [weatherTool] },
  chatFn
);
// LLM calls tool â†’ executes â†’ sends results back â†’ final answer
```

### Built-in Example Tools

| Tool | Purpose | File |
|------|---------|------|
| `get_weather` | External HTTP API demo | `tools/examples/weather.ts` |
| `web_search` | Search result parsing | `tools/examples/search.ts` |
| `retrieve_memory` | Overmind memory integration | `tools/examples/memory.ts` |

### Tool Execution Flow

```
User Query
    â†“
LLM recognizes need for tool
    â†“
LLM returns tool_call with arguments
    â†“
toolEnabledChat validates arguments
    â†“
Execute handler function
    â†“
Return result to LLM
    â†“
LLM synthesizes final answer
```

---

## RAG Pipeline

### Components

```typescript
// 1. Chunk documents
const chunks = chunkMarkdown(documentation, { chunkSize: 500 });

// 2. Generate embeddings
const embeddings = await embedBatch(
  chunks.map(c => c.content),
  { provider: 'ollama', cache: true }
);

// 3. Build vector store
const store = createVectorStore();
for (let i = 0; i < chunks.length; i++) {
  store.add({
    id: chunks[i].id,
    content: chunks[i].content,
    embedding: embeddings[i].vector,
    metadata: chunks[i].metadata,
    timestamp: Date.now()
  });
}

// 4. RAG-enabled chat
const { messages, retrieval } = await ragChat(
  'What are the routing strategies?',
  store,
  originalMessages,
  { contextFormat: 'system', search: { k: 3 } }
);

const response = await chatLLM(messages);
```

### Chunking Strategies

| Strategy | Use Case | Function |
|----------|----------|----------|
| **Character-based** | Generic text | `chunkText()` |
| **Sentence-based** | Semantic boundaries | `chunkBySentence()` |
| **Markdown-aware** | Documentation | `chunkMarkdown()` |

### Embedding Models

| Model | Dimensions | Max Context | Provider | Cost |
|-------|-----------|-------------|----------|------|
| `nomic-embed-text` | 768 | 8,192 | Ollama | $0.00 |
| `text-embedding-3-small` | 1,536 | 8,191 | OpenAI | $0.02/1M |
| `text-embedding-3-large` | 3,072 | 8,191 | OpenAI | $0.13/1M |

---

## Installation & Setup

### Prerequisites

- **Node.js**: 20+
- **Python**: 3.11+
- **pnpm**: 9+
- **Ollama**: Latest stable

### Quick Start (5 minutes)

```bash
# 1. Navigate to Overmind directory
cd GoblinOS/packages/goblins/overmind

# 2. Run automated setup script
./scripts/setup-phase3.sh
# This will:
#   - Check Ollama installation
#   - Pull models (llama3.1, qwen2.5-coder:7b, nomic-embed-text)
#   - Install LiteLLM via pip
#   - Install Node.js dependencies

# 3. Configure environment
cp .env.example .env
# Edit .env and add:
#   OPENAI_API_KEY=sk-...
#   DEEPSEEK_API_KEY=sk-...
#   GEMINI_API_KEY=...

# 4. Start LiteLLM proxy (separate terminal)
litellm --config infra/litellm.config.yaml --port 4000

# 5. Verify Ollama
ollama run llama3.1
# Ask: "What can you do?"
# Type /bye to exit

# 6. Test routing
pnpm build
node -e "import('./src/router/policy.js').then(m => console.log(m.selectProvider(...)))"
```

### Manual Installation

#### Ollama Setup

```bash
# macOS
brew install ollama
ollama serve  # Start service

# Linux
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl start ollama

# Windows
# Download from https://ollama.com/download
```

#### Pull Models

```bash
ollama pull llama3.1           # General chat (4.7GB)
ollama pull qwen2.5-coder:7b   # Code + tools (4.7GB)
ollama pull nomic-embed-text   # Embeddings (274MB)

# Verify
ollama list
```

#### LiteLLM Proxy

```bash
pip install litellm[proxy]

# Start proxy
litellm --config infra/litellm.config.yaml --port 4000 --verbose

# Verify
curl http://localhost:4000/health
```

#### Dependencies

```bash
# Node.js packages
pnpm install

# Python packages (for FastAPI backend)
pip install -r api/requirements.txt
pip install -r api/requirements-otel.txt
```

---

## Migration from Phase 2

### Breaking Changes

1. **New LLMProvider enum values**
   ```typescript
   // OLD
   type Provider = 'openai' | 'deepseek' | 'gemini';

   // NEW
   type Provider = 'openai' | 'deepseek' | 'gemini' | 'ollama' | 'litellm';
   ```

2. **Routing config schema extended**
   ```typescript
   // NEW fields in OvermindConfig.routing
   {
     preferLocal: boolean;      // Default: false
     offlineMode: boolean;      // Default: false
   }
   ```

3. **Provider config additions**
   ```typescript
   // NEW provider configs
   providers: {
     // ... existing providers
     ollama?: {
       baseURL: string;         // Default: http://localhost:11434
       defaultModel: string;    // Default: llama3.1
     };
     litellm?: {
       baseURL: string;         // Default: http://localhost:4000
       apiKey: string;          // Default: 'proxy'
     };
   }
   ```

### Migration Checklist

- [ ] Install Ollama and pull models
- [ ] Install LiteLLM proxy
- [ ] Update `.env.example` with new variables (already done in codebase)
- [ ] Update TypeScript code importing `LLMProvider` enum
- [ ] Test existing routing logic (should still work with defaults)
- [ ] Optionally enable `LOCAL_FIRST` strategy for dev
- [ ] Update deployment configs (Docker, k8s) if using LiteLLM

### Backwards Compatibility

âœ… **Phase 2 code continues to work**:
- Existing provider configurations unchanged
- Default routing strategy remains `cost-optimized`
- New providers are opt-in via environment variables

---

## Usage Examples

### Example 1: Cost-Optimized Development

```typescript
import { selectProvider } from './router/policy';
import { chatOllama } from './clients/ollama-openai';

// Force local-only for zero cost
const provider = selectProvider(config, {
  offlineOnly: true,
  taskType: 'general'
});

// Use Ollama for free inference
const response = await chatOllama([
  { role: 'user', content: 'Explain dependency injection' }
], 'llama3.1');
```

**Cost**: $0.00 ðŸŽ‰

### Example 2: Production Multi-Provider

```typescript
import { chatLiteLLM, selectModel } from './clients/litellm-proxy';

// Smart routing with fallbacks
const model = selectModel({
  taskType: 'creative',
  requireJSON: true,
  maxLatency: 2000,
  maxCost: 1.00
});

const response = await chatLiteLLM(messages, {
  taskType: 'creative',
  preferProvider: 'gemini'  // Try Gemini first, fallback to OpenAI
});
```

**Automatic fallback chain**: Gemini â†’ OpenAI â†’ DeepSeek

### Example 3: Tool-Enabled Coding Assistant

```typescript
import { toolEnabledChat } from './tools';
import { weatherTool, searchTool } from './tools/examples';
import { chatOllamaTools } from './clients/ollama-native';

const messages = [
  { role: 'user', content: 'Create a weather dashboard for Tokyo' }
];

const result = await toolEnabledChat(
  { messages, tools: [weatherTool, searchTool] },
  async (msgs, tools) => {
    return chatOllamaTools({
      messages: msgs,
      tools,
      model: 'qwen2.5-coder:7b'  // Strong tool-calling support
    });
  }
);

console.log('Tools used:', result.tool_calls_made);
// ['get_weather', 'web_search']

console.log('Final code:', result.content);
```

### Example 4: Local RAG Knowledge Base

```typescript
import { createVectorStore, chunkMarkdown, embedBatch, ragChat } from './rag';

// 1. Build knowledge base
const docs = await loadDocumentation();  // Your docs
const chunks = docs.flatMap(doc =>
  chunkMarkdown(doc.content, { chunkSize: 500 }, doc.id)
);

const embeddings = await embedBatch(
  chunks.map(c => c.content),
  { provider: 'ollama', model: 'nomic-embed-text', cache: true }
);

const store = createVectorStore();
chunks.forEach((chunk, i) => {
  store.add({
    id: chunk.id,
    content: chunk.content,
    embedding: embeddings[i].vector,
    metadata: chunk.metadata,
    timestamp: Date.now()
  });
});

// 2. RAG-enabled chat
const query = 'How do I configure routing strategies?';
const { messages } = await ragChat(query, store, [
  { role: 'user', content: query }
], { contextFormat: 'system', search: { k: 3 } });

const response = await chatOllama(messages, 'llama3.1');
```

**Cost**: $0.00 (local embeddings + local LLM)

---

## Troubleshooting

### Ollama Issues

#### `Error: connect ECONNREFUSED 127.0.0.1:11434`

**Cause**: Ollama service not running
**Fix**:
```bash
# Check if running
ps aux | grep ollama

# Start service
ollama serve

# Or restart
pkill ollama && ollama serve
```

#### `Model not found: llama3.1`

**Cause**: Model not pulled
**Fix**:
```bash
ollama pull llama3.1
ollama list  # Verify
```

#### Slow inference (>10s per request)

**Causes**:
- Model not fitting in VRAM
- CPU-only inference
- Large context window

**Fixes**:
```bash
# Check GPU usage
nvidia-smi  # Linux
system_profiler SPDisplaysDataType  # macOS

# Use smaller model
ollama pull llama3.1:7b  # Instead of 70b

# Reduce context
# In code: max_tokens: 2048 instead of 4096
```

### LiteLLM Proxy Issues

#### `502 Bad Gateway` from proxy

**Cause**: Proxy can't reach provider
**Fix**:
```bash
# Check proxy logs
litellm --config infra/litellm.config.yaml --debug

# Verify API keys
echo $OPENAI_API_KEY
echo $GEMINI_API_KEY
```

#### `Rate limit exceeded`

**Cause**: Too many requests to cloud providers
**Fix**:
```yaml
# infra/litellm.config.yaml
general_settings:
  max_parallel_requests: 5      # Reduce concurrency
  allowed_fails: 3
  cooldown_time: 120            # Increase cooldown
```

### TypeScript Compilation Errors

#### `Cannot find module 'openai'`

**Cause**: Missing dependency
**Fix**:
```bash
pnpm install openai@^4.68.0
```

#### `process.env is not defined`

**Cause**: Missing Node.js types
**Fix**:
```bash
pnpm add -D @types/node@^20.0.0
```

#### Import errors with `.js` extensions

**Cause**: ESM module resolution
**Fix**: Ensure `tsconfig.json` has:
```json
{
  "compilerOptions": {
    "module": "node16",
    "moduleResolution": "node16"
  }
}
```

---

## Performance Benchmarks

### Latency Comparison (p50 / p95)

| Task | Ollama | DeepSeek | Gemini | OpenAI |
|------|--------|----------|--------|--------|
| **Simple query** | 1.8s / 3.2s | 0.6s / 1.1s | 1.0s / 1.8s | 1.2s / 2.1s |
| **Code generation** | 2.5s / 4.5s | 0.9s / 1.5s | 1.4s / 2.3s | 1.6s / 2.8s |
| **JSON output** | 2.1s / 3.8s | 0.7s / 1.3s | 1.1s / 2.0s | 1.4s / 2.4s |
| **Tool calling** | 3.0s / 5.2s | 1.2s / 2.0s | 1.6s / 2.7s | 1.8s / 3.1s |
| **Embeddings** | 0.3s / 0.5s | N/A | N/A | 0.2s / 0.4s |

*Tested on M2 Mac (Ollama), cloud providers (us-west-2)*

### Cost Comparison (1M tokens)

| Provider | Input | Output | Average |
|----------|-------|--------|---------|
| Ollama | $0.00 | $0.00 | $0.00 |
| DeepSeek | $0.07 | $0.28 | $0.14 |
| Gemini | $0.15 | $0.60 | $0.30 |
| OpenAI | $1.25 | $5.00 | $2.50 |

### Throughput (requests/minute)

| Setup | Max RPS | Notes |
|-------|---------|-------|
| Ollama (M2 Mac) | 5-8 | Limited by CPU/GPU |
| Ollama (A100 GPU) | 30-50 | GPU-accelerated |
| DeepSeek (cloud) | 100+ | Rate limits apply |
| LiteLLM (multi-provider) | 200+ | Load balanced |

---

## API Reference

### Router Policy

```typescript
function selectProvider(
  config: OvermindConfig,
  requirements: TaskRequirements,
  providerHealth?: Map<LLMProvider, HealthStatus>
): {
  provider: LLMProvider;
  reason: string;
  model: string;
}
```

**Parameters**:
- `config`: Overmind configuration with provider settings
- `requirements`: Task requirements (type, complexity, capabilities)
- `providerHealth`: Optional health status map

**Returns**: Selected provider with reasoning

### Ollama Clients

```typescript
// OpenAI-compatible
function chatOllama(
  messages: ChatCompletionMessageParam[],
  model?: string,
  options?: { temperature?: number; max_tokens?: number }
): Promise<ChatCompletion>

// Native client - Structured outputs
function chatOllamaStructured(
  request: StructuredOutputRequest
): Promise<ChatResponse>

// Native client - Tool calling
function chatOllamaTools(
  request: ToolCallRequest
): Promise<ChatResponse>

// Embeddings
function embedOllama(
  text: string,
  model?: string
): Promise<number[]>
```

### LiteLLM Proxy

```typescript
function selectModel(
  preferences: RoutingPreferences
): string

function chatLiteLLM(
  messages: ChatCompletionMessageParam[],
  preferences?: RoutingPreferences,
  options?: ChatOptions
): Promise<ChatCompletion>

function embedLiteLLM(
  text: string | string[],
  preferences?: RoutingPreferences
): Promise<number[] | EmbeddingResponse>
```

### Tool System

```typescript
interface ToolDefinition {
  name: string;
  description: string;
  parameters: JSONSchema;
  handler: (args: Record<string, unknown>) => Promise<string> | string;
}

function toolEnabledChat(
  request: ToolChatRequest,
  chatFn: ChatFunction,
  maxIterations?: number
): Promise<{
  content: string;
  iterations: number;
  tool_calls_made: string[];
}>
```

### RAG System

```typescript
// Embeddings
function embed(
  text: string,
  config?: EmbeddingConfig
): Promise<EmbeddingResult>

function embedBatch(
  texts: string[],
  config?: EmbeddingConfig
): Promise<EmbeddingResult[]>

// Vector Store
class VectorStore {
  add(doc: Document): void;
  search(queryEmbedding: number[], options?: SearchOptions): SearchResult[];
  toJSON(): string;
  fromJSON(json: string): void;
}

// Retrieval
function retrieve(
  query: string,
  vectorStore: VectorStore,
  config?: RetrievalConfig
): Promise<RetrievalResult>

function ragChat(
  query: string,
  vectorStore: VectorStore,
  messages: ChatCompletionMessageParam[],
  config?: RetrievalConfig
): Promise<{
  messages: ChatCompletionMessageParam[];
  retrieval: RetrievalResult;
}>

// Chunking
function chunkText(
  text: string,
  config?: ChunkingConfig,
  sourceId?: string
): Chunk[]

function chunkMarkdown(
  markdown: string,
  config?: ChunkingConfig,
  sourceId?: string
): Chunk[]
```

---

## What's Next

### Completed (Phase 3)
- âœ… Ollama integration (3 adapters)
- âœ… Smart routing with 6 strategies
- âœ… Tool calling framework
- âœ… Local RAG pipeline
- âœ… LiteLLM proxy integration

### Future Enhancements

**Phase 4 (Planned)**:
- ðŸ”„ Dashboard UI (model manager, analytics, RAG explorer)
- ðŸ”„ Cross-encoder re-ranking for RAG
- ðŸ”„ Persistent vector DB (Chroma/Qdrant)
- ðŸ”„ Multi-modal support (images, audio)
- ðŸ”„ Streaming tool execution
- ðŸ”„ Agent orchestration (multi-agent workflows)

---

## Support & Contributing

**Documentation**: `/docs/` directory
**Issues**: Create GitHub issue with `phase-3` label
**Examples**: `src/tools/examples.ts`, `src/rag/examples.ts`

**Maintainers**: @fuaadabdullah
**Last Updated**: October 25, 2025
